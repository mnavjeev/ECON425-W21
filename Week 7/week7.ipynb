{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 7: Neural Networks with TensorFlow\n",
    "\n",
    "This week we will go over how to use TensorFlow to implement a neural network\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "source": [
    "For now, we will be working with the Sequential neural network class in keras. A sequential neural net is one where each layer takes in a single set of inputs and outputs a single set of outputs. When our input is a vector (as has been the case so far) we can think of this as each layer taking in a single input matrix and outputing a single output matrix, where the input matrix has shape \n",
    "$$\\text{(number of neurons in past layer)} \\times \\text{(number of neurons in this layer)}$$\n",
    "and the output matrix has shape\n",
    "$$\\text{(number of neurons in this layer)} \\times \\text{(number of neurons in next layer)}$$\n",
    "Non zero entries in these matrices will indicate that the output from a neuron in a prior layer is being passed to a neuron in the next layer.\n",
    "\n",
    "We can initialize this neural network using `model = keras.Sequential()`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "source": [
    "Now that we have initiallized our model as a sequential neural network, we want to start to add layers to this neural net. To initialize a dense layer we can use \n",
    "\n",
    "``layers.Dense(num_neurons)``\n",
    "\n",
    "A Dense layer means each neuron will take in all the inputs from the past layer (input matrix is fully non-zero). There are a few other options we can use when initializing a layer:\n",
    "\n",
    "`` use_bias = True/False``\n",
    "\n",
    "This specifies whether the neurons in this layer will use a bias term. By default use_bias is set to be True. \n",
    "\n",
    "`` activation = \"relu\"/\"sigmoid\"/etc.``\n",
    "\n",
    "This specifies what activation function is used in the layer. If this is left unspecified, then we will use a linear activation function $\\pi(x) = x$.\n",
    "\n",
    "We also may want to specify the shape of the initial input. We can do this using \n",
    "\n",
    "``keras.Input(shape = ())``\n",
    "\n",
    "In this particular case we will be using 4 features to clasify penguins, so we specify our input shape as (4,)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.Input(shape = (4,))\n",
    "layer1 = layers.Dense(2, use_bias = True, activation = \"sigmoid\")\n",
    "layer2 = layers.Dense(10, use_bias = True, activation = \"sigmoid\")\n",
    "layer3 = layers.Dense(20, use_bias = False)\n",
    "output = layers.Dense(1, use_bias = True, activation = \"sigmoid\")"
   ]
  },
  {
   "source": [
    "Once we have set up our layers, we can add them to our neural network using `model.add()`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(input)\n",
    "model.add(layer1)\n",
    "model.add(layer2)\n",
    "model.add(layer3)"
   ]
  },
  {
   "source": [
    "We can remove the last layer by using `model.pop()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n2\n"
     ]
    }
   ],
   "source": [
    "print(len(model.layers))\n",
    "model.pop()\n",
    "print(len(model.layers))\n",
    "\n"
   ]
  },
  {
   "source": [
    "At this point we can add the output layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(output)"
   ]
  },
  {
   "source": [
    "Our model is now initialized. We can use the `model.summary()` model to take a look at the layers in our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_62 (Dense)             (None, 2)                 10        \n_________________________________________________________________\ndense_63 (Dense)             (None, 10)                30        \n_________________________________________________________________\ndense_65 (Dense)             (None, 1)                 11        \n=================================================================\nTotal params: 51\nTrainable params: 51\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "source": [
    "and the `model.weights` attribute to see what (random) weights the model has been initialized with"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_62/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
       " array([[ 0.5941806 , -0.3471973 ],\n",
       "        [ 0.07243395, -0.09919524],\n",
       "        [-0.58444524, -0.05843472],\n",
       "        [ 0.00209427, -0.34695196]], dtype=float32)>,\n",
       " <tf.Variable 'dense_62/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_63/kernel:0' shape=(2, 10) dtype=float32, numpy=\n",
       " array([[-0.66048074, -0.62314504, -0.23081267, -0.5326394 , -0.07129222,\n",
       "          0.6264935 , -0.35728034, -0.39815134,  0.1333983 ,  0.49193138],\n",
       "        [ 0.7062308 , -0.08162123,  0.4638719 ,  0.6584328 ,  0.58804685,\n",
       "          0.34665996, -0.53175616,  0.20638853, -0.60973036, -0.3571642 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_63/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_65/kernel:0' shape=(10, 1) dtype=float32, numpy=\n",
       " array([[ 0.6257991 ],\n",
       "        [-0.34274462],\n",
       "        [ 0.6847772 ],\n",
       "        [-0.49821234],\n",
       "        [ 0.37540072],\n",
       "        [-0.48557815],\n",
       "        [-0.0414505 ],\n",
       "        [-0.568015  ],\n",
       "        [-0.72617304],\n",
       "        [ 0.1030944 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_65/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "source": [
    "We can see what predictions our model would make given these weights by just pasing it a single 4 element vector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([[0.3925631]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "penguins = sns.load_dataset('penguins').dropna()\n",
    "penguins.head()\n",
    "Y = 1*(penguins['species']==\"Adelie\")\n",
    "X = penguins[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']]\n",
    "test = np.array(X[0:1])\n",
    "y = model(test)\n",
    "print(y)\n"
   ]
  },
  {
   "source": [
    "Now, we want to move on to training our neural network using the data. We first split our data into testing and training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y)"
   ]
  },
  {
   "source": [
    "We first specify a training configuration using \n",
    "\n",
    "``model.compile(optimizer, loss, metrics)``\n",
    "\n",
    "The optimizer tells keras how to numerically compute the derivative at each step of the back-popogation. Examples include ``keras.optimizers.SGD()``, ``keras.optimizers.Adam()``, or ``keras.optimizers.RMSprop()``. For now we use Adam, which implements a stochastic gradient descent estimating the first and second derivatives at each step of the back propogation.\n",
    "\n",
    "The loss function tells us what loss function we want to minimize when doing back-propogation. Examples include ``keras.losses.MeanSquaredError()``, ``keras.losses.KLDivergence()``, etc. For this example we will use the MSE.\n",
    "\n",
    "Finally, the metrics argument tells what metrics to use to describe our models performance on the training data. Examples include ``keras.metrics.Accuracy()``, ``keras.metrics.Crossentropy()``, etc. We can pass multiple metrics to this argument. For this example we will use the KL Divergence.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.KLDivergence()])"
   ]
  },
  {
   "source": [
    "Finally, we fit the model using\n",
    "\n",
    "``model.fit(Xtrain, Ytrain, epochs)``\n",
    "\n",
    "When training the model keras will try to split up the data into batches (randomly) and do back-progogation to compute the gradient on each subsample. Epochs tells the data how many times to iterate through and use the whole dataset and run back-propogation. For now we try 50 epochs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 905us/step - loss: 0.2552 - kullback_leibler_divergence: 0.4384\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 736us/step - loss: 0.2540 - kullback_leibler_divergence: 0.4295\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 871us/step - loss: 0.2533 - kullback_leibler_divergence: 0.4197\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 925us/step - loss: 0.2521 - kullback_leibler_divergence: 0.4099\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 911us/step - loss: 0.2514 - kullback_leibler_divergence: 0.4021\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2508 - kullback_leibler_divergence: 0.3948\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 979us/step - loss: 0.2505 - kullback_leibler_divergence: 0.3879\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 969us/step - loss: 0.2501 - kullback_leibler_divergence: 0.3819\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2498 - kullback_leibler_divergence: 0.3766\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2497 - kullback_leibler_divergence: 0.3736\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 952us/step - loss: 0.2496 - kullback_leibler_divergence: 0.3702\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2495 - kullback_leibler_divergence: 0.3670\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 974us/step - loss: 0.2494 - kullback_leibler_divergence: 0.3648\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 915us/step - loss: 0.2494 - kullback_leibler_divergence: 0.3626\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3603\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3580\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3571\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3563\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3558\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3558\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 994us/step - loss: 0.2494 - kullback_leibler_divergence: 0.3561\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3555\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3551\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3546\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3544\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.2494 - kullback_leibler_divergence: 0.3558\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3545\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3550\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3542\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3529\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 962us/step - loss: 0.2493 - kullback_leibler_divergence: 0.3535\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3537\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3538\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3537\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3543\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3551\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 928us/step - loss: 0.2493 - kullback_leibler_divergence: 0.3546\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3541\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3527\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3528\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3541\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3541\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3548\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3546\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3535\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.2493 - kullback_leibler_divergence: 0.3534\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3533\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2493 - kullback_leibler_divergence: 0.3527\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 977us/step - loss: 0.2493 - kullback_leibler_divergence: 0.3528\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - kullback_leibler_divergence: 0.3537\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f86f16048d0>"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, epochs = 50)"
   ]
  },
  {
   "source": [
    "Using the predict method we can make predictions and validate this neural net using our testing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f86eeda4ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "Yhat = 1*(model.predict(Xtest).flatten() >= 0.5)\n",
    "print(np.mean(Yhat == Ytest))\n"
   ]
  },
  {
   "source": [
    "Our model preforms just ok. But, we did get it to work."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}