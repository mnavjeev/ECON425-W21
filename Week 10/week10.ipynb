{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "108527f9cb1bb1ccc1ec9b7ead991ffae931b29776782b7d6313705e7dd6f31e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 10: Bagging and Boosting\n",
    "\n",
    "\n",
    "Up till now, we have used our training data to traing a single model over the training set and applying it to the testing data. However, in order to reduce the final variance of our estimator, we may want to train multiple models and aggregate the results of these models. This will make our final prediction less vulnerable to overfitting and hopefully more accurate. This is the idea behind bagging and boosting. Today in section we will cover bagging and boosting as well as how to implement them in python.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Bagging: Bootstrap Aggregating\n",
    "\n",
    "Suppose our training data consists of $N$ samples. A bootstrap sample is also a sample of size $K$ drawn from our training sample with replacement. The idea behind bagging is to draw some $B$ bootstrap samples from our training data, estimate a model on each of our samples, and then average them in some fashion (either a simple average or take a weighted average).\n",
    "\n",
    "Below, we will show how to do this in a very simple fashion with a linear regression model to get the basic idea down. Then, we will use a build in sklearn library to show how this can be generalized. To do so we will use the 'mpg' dataset from seaborn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0   \n",
       "1  15.0          8         350.0       165.0    3693          11.5   \n",
       "2  18.0          8         318.0       150.0    3436          11.0   \n",
       "3  16.0          8         304.0       150.0    3433          12.0   \n",
       "4  17.0          8         302.0       140.0    3449          10.5   \n",
       "\n",
       "   model_year origin                       name  one  \n",
       "0          70    usa  chevrolet chevelle malibu    1  \n",
       "1          70    usa          buick skylark 320    1  \n",
       "2          70    usa         plymouth satellite    1  \n",
       "3          70    usa              amc rebel sst    1  \n",
       "4          70    usa                ford torino    1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model_year</th>\n      <th>origin</th>\n      <th>name</th>\n      <th>one</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>18.0</td>\n      <td>8</td>\n      <td>307.0</td>\n      <td>130.0</td>\n      <td>3504</td>\n      <td>12.0</td>\n      <td>70</td>\n      <td>usa</td>\n      <td>chevrolet chevelle malibu</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>15.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165.0</td>\n      <td>3693</td>\n      <td>11.5</td>\n      <td>70</td>\n      <td>usa</td>\n      <td>buick skylark 320</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>18.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>3436</td>\n      <td>11.0</td>\n      <td>70</td>\n      <td>usa</td>\n      <td>plymouth satellite</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>16.0</td>\n      <td>8</td>\n      <td>304.0</td>\n      <td>150.0</td>\n      <td>3433</td>\n      <td>12.0</td>\n      <td>70</td>\n      <td>usa</td>\n      <td>amc rebel sst</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>17.0</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>140.0</td>\n      <td>3449</td>\n      <td>10.5</td>\n      <td>70</td>\n      <td>usa</td>\n      <td>ford torino</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "mpg = sns.load_dataset(\"mpg\").dropna()\n",
    "mpg['one'] = 1\n",
    "mpg.head()"
   ]
  },
  {
   "source": [
    "We will use the displacement, cylinders horsepower, weight, and acceleration to try and predict the mpg of the vehicle. To illustrate the usefuleness, we will only try to make the prediction for cars in the year 1970 and 1971. We first fit the model the way we are used to doing so. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.9501068589016715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "mpg70 = mpg[mpg['model_year']<=71]\n",
    "X = mpg70[['one','cylinders','displacement','horsepower','weight','acceleration']]\n",
    "Y = mpg70[['mpg']]\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y, random_state = 1)\n",
    "\n",
    "wholeModel = LinearRegression(fit_intercept = False)\n",
    "wholeModel.fit(Xtrain,Ytrain)\n",
    "\n",
    "Yhat = wholeModel.predict(Xtest)\n",
    "whole_mse = np.mean((Ytest - Yhat)**2)\n",
    "print(float(whole_mse))\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "For each of the $B$ models we want to estimate we will do the following:\n",
    "\n",
    "1. Draw a bootstrap sample of size $K$. \n",
    "2. Estimate the model on our boostrap sample.\n",
    "3. Store the coeffecients of our model.\n",
    "\n",
    "To come up with our final model, we will take a weighted average of our model coeffecients, weighting by the inverse of the out of sample MSE."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 4.49018213e+01 -1.58077817e+00 -4.20839817e-03 -2.47926297e-03\n -3.44682985e-03 -2.25247906e-01]\n"
     ]
    }
   ],
   "source": [
    "def baggedLR(Xtr, Xts, Ytr, Yts, B, K_prop):\n",
    "    N = Xtr.shape[0]\n",
    "    K = round(K_prop*N)\n",
    "    training = pd.concat([Xtr,Ytr], axis = 1)\n",
    "    mse = []\n",
    "    coeffecients = []\n",
    "    for b in range(B):\n",
    "        bsample = training.sample(K, replace = True)\n",
    "        bx = bsample[['one','cylinders','displacement','horsepower','weight','acceleration']]\n",
    "        by = bsample[['mpg']]\n",
    "        bmodel = LinearRegression(fit_intercept=False)\n",
    "        bmodel.fit(bx,by)\n",
    "        bYhat = bmodel.predict(Xts)\n",
    "        bmse = float(np.mean((bYhat - Yts)**2))\n",
    "        bcoef = bmodel.coef_\n",
    "        mse.append(bmse)\n",
    "        coeffecients.append(bcoef)\n",
    "    final = np.average(coeffecients, axis = 0, weights = 1/np.array(mse))[0]\n",
    "    return final\n",
    "\n",
    "test = baggedLR(Xtrain, Xtest, Ytrain, Ytest, 1000, 1)\n",
    "print(test)"
   ]
  },
  {
   "source": [
    "We can now evaluate the performance of this model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.6195007663030045\n"
     ]
    }
   ],
   "source": [
    "baggedHat = np.dot(Xtest, test)\n",
    "baggedMSE = np.mean((np.array(Ytest).flatten() - baggedHat)**2)\n",
    "print(baggedMSE)"
   ]
  },
  {
   "source": [
    "### The SKlearn Bagging Ensemble Method\n",
    "\n",
    "We can see that we have reduced the MSE of our model by bagging $^1$. Next we will go over how we can use the build in sklearn bagging classifier and estimator to do this automatically. The sklearn feature will even have some more functionality that we did not build into this simple model above.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$^1$ *Though this example is a bit contrived since we are not updating the testing dataset each time*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n"
   ]
  },
  {
   "source": [
    "The basic syntax of the BaggingClassifier or BaggingRegressor is as follows:\n",
    "\n",
    "`` RegressionModel = BaggingRegressor(base_regression_model, n_estimators = B, bootstrap = True/False, oob_score = True/False, max_samples = K, max_features = m)``\n",
    "\n",
    "`` ClassifierModel = BaggingRegressor(base_classification_model, n_estimators = B, bootstrap = True/False, oob_score = True/False,max_samples = K, max_features = m)``\n",
    "\n",
    "In Order:\n",
    "\n",
    "``base_regression_model/base_classification_model``: Default is a decision tree classifier/regressor. Here we could specify a `LinearRegression()` or a `KNearestNeighborsClassifier()`.\n",
    "\n",
    "`n_estimators`: Specifies how many models we should aggregate to get our final model. Like $B$ in the above, the number of times we draw a bootstrap sample and estimate the model on that sample. \n",
    "\n",
    "`bootstrap`: Whether the bootstrap samples should be sampled with or without replacement. For this class, we will generally keep this at `True`\n",
    "\n",
    "`oob_score`: Whether to calculate the MSE/mean accuracy on the left out samples and use this to weight bootstrap models in the final aggregation\n",
    "\n",
    "`max_sample`: We can think of this as $K$ in the above. The size of each of our bootstrap samples. (Default is $N$)\n",
    "\n",
    "`max_features`: This is new to the sklearn bagging method. This limits the complexity of each of the bootstrap models. If max_features is set to something lower than the total number of features the bagging method will randomly sample features from the data.\n",
    "\n",
    "Lets see how this works below:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_bagged = BaggingRegressor(LinearRegression(fit_intercept=False), n_estimators= 100, bootstrap=True, oob_score=True, max_features = 6, random_state=2)"
   ]
  },
  {
   "source": [
    "Now that we've set up our bagging model, we can fit and predict the model as we're used to"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.865413133529548\n"
     ]
    }
   ],
   "source": [
    "sk_bagged.fit(Xtrain,np.array(Ytrain).ravel())\n",
    "sk_bagHat = sk_bagged.predict(Xtest)\n",
    "sk_bagMSE = np.mean((sk_bagHat-np.array(Ytest).ravel())**2)\n",
    "print(sk_bagMSE)"
   ]
  },
  {
   "source": [
    "This does a little bit wose than the bagging estimator we estimated ourselves (which is to be expected since we were fitting to the testing data), but a bit better than the non-bagged estimator."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To see how this works with classification, lets go back to KNearestNeighbors and the diamonds dataset. We are interested in classifying the cut of a diamond."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       cut  carat  depth  price\n",
       "0    Ideal   0.23   61.5    326\n",
       "1  Premium   0.21   59.8    326\n",
       "2     Good   0.23   56.9    327\n",
       "3  Premium   0.29   62.4    334\n",
       "4     Good   0.31   63.3    335"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cut</th>\n      <th>carat</th>\n      <th>depth</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Ideal</td>\n      <td>0.23</td>\n      <td>61.5</td>\n      <td>326</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Premium</td>\n      <td>0.21</td>\n      <td>59.8</td>\n      <td>326</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Good</td>\n      <td>0.23</td>\n      <td>56.9</td>\n      <td>327</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Premium</td>\n      <td>0.29</td>\n      <td>62.4</td>\n      <td>334</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>Good</td>\n      <td>0.31</td>\n      <td>63.3</td>\n      <td>335</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "diamonds = sns.load_dataset('diamonds')[['cut','carat','depth','price']]\n",
    "diamonds.head()"
   ]
  },
  {
   "source": [
    "First, to review, we fit the KNeighborsClassifier without bagging."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6419725621060437\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "X = diamonds[['carat','depth','price']]\n",
    "Y = diamonds['cut']\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y, random_state = 0)\n",
    "\n",
    "knn = KNN(n_neighbors=5)\n",
    "knn.fit(Xtrain,Ytrain)\n",
    "yHat = knn.predict(Xtest)\n",
    "accuracy = np.mean(1*(yHat == Ytest))\n",
    "\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "source": [
    "We now \"bag\" (bootstrap aggregate) this classifier using the same syntax as above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6505747126436782\n"
     ]
    }
   ],
   "source": [
    "bagged_knn = BaggingClassifier(KNN(n_neighbors=5), n_estimators=100, max_samples=0.5, oob_score=True)\n",
    "bagged_knn.fit(Xtrain,Ytrain)\n",
    "bagged_yHat = bagged_knn.predict(Xtest)\n",
    "accuracy = np.mean(1*(bagged_yHat == Ytest))\n",
    "\n",
    "print(accuracy)\n",
    "\n"
   ]
  },
  {
   "source": [
    "The bagged classifier does a bit better in practice."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Boosting: Sequential Model Search\n",
    "\n",
    "Boosting can be thought of as an extension of the ideas behind bagging. Boosting works by aggregating together a bunch of simple models to come up with a more complex (and hopefully more accurate) model. Each model is fitted in a succesively, with observations with high errors in prior models being given larger weight in succesive models.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor"
   ]
  },
  {
   "source": [
    "The basic syntax of the AdaBoosting classifier/regressor is as follows \n",
    "\n",
    "``RegressionModel = AdaBoostRegressor(base_estimator, n_estimators, learning_rate, loss)``\n",
    "\n",
    "``ClassificationModel = AdaBoostClassifier(base_estimator, n_estimators, learning_rate)``\n",
    "\n",
    "In order:\n",
    "\n",
    "`base_estimator`: Here we could specify a `LinearRegression()` or a `KNearestNeighborsClassifier()`. It is the simple model that the boosting aggregator ultimately aggregates over.\n",
    "\n",
    "`n_estimators`: Number of times the algorithm will fit a model and update the sample weights for the next model\n",
    "\n",
    "`learning_rate`: Governs how fast the weights are updated after each iteration. This is a tuning parameter that we will have to pick\n",
    "\n",
    "`loss`: Only for regression models. Governs the loss used when updating the weights. Options are linear, square, or exponential.\n",
    "\n",
    "First, let's see how this works on the regression model.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.9501068589016715\n"
     ]
    }
   ],
   "source": [
    "mpg70 = mpg[mpg['model_year']<=71]\n",
    "X = mpg70[['one','cylinders','displacement','horsepower','weight','acceleration']]\n",
    "Y = mpg70['mpg']\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y, random_state = 1)\n",
    "\n",
    "wholeModel = LinearRegression(fit_intercept = False)\n",
    "wholeModel.fit(Xtrain,Ytrain)\n",
    "\n",
    "Yhat = wholeModel.predict(Xtest)\n",
    "whole_mse = np.mean((Ytest - Yhat)**2)\n",
    "print(float(whole_mse))"
   ]
  },
  {
   "source": [
    "boosted_reg = AdaBoostRegressor(LinearRegression(fit_intercept=False), n_estimators=100, learning_rate= 0.8, loss = 'linear', random_state = 2)\n",
    "boosted_reg.fit(Xtrain, Ytrain)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 170,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=LinearRegression(copy_X=True,\n",
       "                                                  fit_intercept=False,\n",
       "                                                  n_jobs=None,\n",
       "                                                  normalize=False),\n",
       "                  learning_rate=0.8, loss='linear', n_estimators=100,\n",
       "                  random_state=2)"
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ]
  },
  {
   "source": [
    "We can now predict using our model and evaluate as before"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.132719985929552\n"
     ]
    }
   ],
   "source": [
    "yHat = boosted_reg.predict(Xtest)\n",
    "boosted_mse = np.mean((Ytest-yHat)**2)\n",
    "print(boosted_mse)"
   ]
  },
  {
   "source": [
    "Now revist the classification problem from before:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6419725621060437\n"
     ]
    }
   ],
   "source": [
    "X = diamonds[['carat','depth','price']]\n",
    "Y = diamonds['cut']\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y, random_state = 0)\n",
    "\n",
    "knn = KNN(n_neighbors=5)\n",
    "knn.fit(Xtrain,Ytrain)\n",
    "yHat = knn.predict(Xtest)\n",
    "accuracy = np.mean(1*(yHat == Ytest))\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "source": [
    "And let's try the boosted classifer. If we leave the base_model unspecified, the bosted classifier will use a decision tree, so let's try that. (Boosting won't work out of the box with KNN since there are no weights to update)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7233222098628105\n"
     ]
    }
   ],
   "source": [
    "boosted_knn = AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=0)\n",
    "boosted_knn.fit(Xtrain,Ytrain)\n",
    "boosted_yHat = boosted_knn.predict(Xtest)\n",
    "boosted_accuracy = np.mean(1*(boosted_yHat == Ytest))\n",
    "print(boosted_accuracy)"
   ]
  },
  {
   "source": [
    "This does a fair bit better than the original (non-boosted) model, and even does better than the bagged esimtator. Though part of this could be due to the different algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}