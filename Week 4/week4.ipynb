{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 4: Numerical Methods\n",
    "\n",
    "Training or fitting a machine learning algorithm often involves solving a minimization problem of some form. For example, given a penalty parameter $\\lambda$, LASSO requires solving the following minimization problem for the parameters $\\beta_0, \\beta_1,\\dots, \\beta_p$:\n",
    "\n",
    "$$ \\beta_0, \\beta_1, \\dots,\\beta_p = \\arg\\min_{b_0,b_1,\\dots,b_p} \\sum_{i=1}^n \\left(Y_i - b_0 - b_1 X_{1,i}-b_2 X_{2,i} - \\dots -\\beta_p X_{p,i}\\right)^2 + \\lambda\\sum_{k=1}^p |b_k|$$\n",
    "\n",
    "In the case that $\\lambda = 0$ (even when $p >> n$!), this minimization has a closed form solution that is effeciently computed. However, in general with machine learning models a closed form formula for the solution is not available. Moreover, we are often fitting these machine learning models on large datasets or fitting them many times to choose our hyperparameters. So, it is important that we learn effecient numerical methods to solve minmization problems and know when these methods work well.\n",
    "\n",
    "\n",
    "## Gradient Descent: Concepts\n",
    "\n",
    "### An overview of minimization \n",
    "\n",
    "In general, we will choose the parameters of our model to minize some (generally convex) cost function that depends on the data. \n",
    "$$ \\theta_0= \\arg\\min_{\\theta} C(\\theta; Y,X)$$\n",
    "I review some basics of minimization below. \n",
    "\n",
    "Suppose I am given a twice continuously-differentiable function $f(x)$. I want to solve for the value, $x_0$, that minimizes $f$. In order to check that $x_0$ is a local minimum for $f$, we need to check the following first and second order conditions:\n",
    "\\begin{align}\n",
    "    f'(x) &= 0 \\\\\n",
    "    f''(x) &> 0\n",
    "\\end{align}\n",
    "If there are multiple points that satisfy the above conditions, we will have to compare the value of $f$ at each of these points to find the true global minimum.\n",
    "\n",
    "However, if $f$ is a convex function, then we know that $f''(x) > 0$ for all values of $x$. Moreover, we know that there can only be one point $x_0$  such that $f'(x_0)=0$. In this case, solving for a global minimum of $f$ simply requires finding the one point $x_0$ that satisfies the first order condition:\n",
    "$$ f'(x_0) = 0$$\n",
    "If $f$ is well behaved then we can explicitly solve for an inverse for $f'$, that is we can find $f'^{-1}$ and solve for:\n",
    "$$ f'^{-1}(0) = x_0$$\n",
    "For example, suppose $f(x) = x^2 + x$. We can easily verify that $f''(x) = 2 > 0$ so that $f$ is a convex function. Then $f'(x) = 2x + 1$ has a well defined inverse, $f'^{-1}(y) = \\frac{y-1}{2}$. So, in order to solve for the minimum of $f(x)$, we can solve for $x_0$:\n",
    "$$f'^{-1}(0) = \\frac{0-1}{2} = -\\frac{1}{2}$$\n",
    "\n",
    "In the case of simple linear regression, the cost function:\n",
    "\n",
    "$$C(\\theta; Y,X) = \\sum_{i=1}^n \\left(Y_i - \\theta_0 - \\theta_1 X_{1,i} - \\dots - \\theta_p X_{p,i}\\right)^2$$\n",
    "\n",
    "Is such that the derivative of the cost function is linear in $\\theta$ and has a well defined inverse. We can solve for $\\theta_0$ via:\n",
    "$$\\theta_0 = E_n\\left[\\mathbf{XX'}\\right]^{-1}E_n\\left[\\mathbf{X'Y}\\right]$$\n",
    "where $\\mathbf{X}$ is a matrix of of our covariates and $\\mathbf{Y}$ is a vector of our outcomes. \n",
    "\n",
    "However, for regularized or penalized cost functions, like in LASSO, Ridge Regression, or Elastic Net, there is no well defined formula for the inverse of the derivative of the cost function. So, in order to solve these minimization problems we will need to numerically find the point at which the derivative of the cost function is equal to 0. The algorithm we will review to help us do this is called gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "### Gradient Descent: Method and Implementation\n",
    "\n",
    "\n",
    "The gradient of a multidimensional function $F: \\mathbb{R}^p \\rightarrow \\mathbb{R}$ is just a vector of it's derivatives w.r.t all components of $x$, that is: \n",
    "\n",
    "$$ \\nabla F(\\bar x) = \\begin{pmatrix} \\frac{\\partial F}{\\partial x_1}(\\bar x) \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial x_p}(\\bar x)\\end{pmatrix}$$\n",
    "\n",
    "To minimize a multidimensional convex function it is necessary and suffecient to find the point at which the gradient is 0.\n",
    "\n",
    "\n",
    "Gradient descent is founded off the observation from multivariable calculus that a multivariable function $F(x): \\mathbb{R}^p \\rightarrow \\mathbb{R}$, that we may be trying to minimize, decreases the fastest at a point $\\bar x$ if one goes in the direction of the negative gradient: $-\\nabla F(\\bar x)$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}