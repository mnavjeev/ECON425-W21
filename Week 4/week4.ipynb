{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 4: Numerical Methods\n",
    "\n",
    "Training or fitting a machine learning algorithm often involves solving a minimization problem of some form. For example, given a penalty parameter $\\lambda$, LASSO requires solving the following minimization problem for the parameters $\\beta_0, \\beta_1,\\dots, \\beta_p$:\n",
    "\n",
    "$$ \\beta_0, \\beta_1, \\dots,\\beta_p = \\arg\\min_{b_0,b_1,\\dots,b_p} \\sum_{i=1}^n \\left(Y_i - b_0 - b_1 X_{1,i}-b_2 X_{2,i} - \\dots -\\beta_p X_{p,i}\\right)^2 + \\lambda\\sum_{k=1}^p |b_k|$$\n",
    "\n",
    "In the case that $\\lambda = 0$ (even when $p >> n$!), this minimization has a closed form solution that is effeciently computed. However, in general with machine learning models a closed form formula for the solution is not available. Moreover, we are often fitting these machine learning models on large datasets or fitting them many times to choose our hyperparameters. So, it is important that we learn effecient numerical methods to solve minmization problems and know when these methods work well.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}